{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np \n",
    "import torch \n",
    "from torchvision import transforms, models \n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "resnet_model = models.resnet50(weights = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.8.1\n"
     ]
    }
   ],
   "source": [
    "print(cv2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "8677\n"
     ]
    }
   ],
   "source": [
    "dataset = torchvision.datasets.Caltech101('/Users/abhinav/Desktop/CSE515-Project', download=True)\n",
    "label_name_to_idx = {name: idx for idx, name in enumerate(dataset.categories)}\n",
    "label_to_name = {value: key for key, value in label_name_to_idx.items()}\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(dataset,\n",
    "batch_size=4,\n",
    "shuffle=True,\n",
    "num_workers=8)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Faces',\n",
       " 1: 'Faces_easy',\n",
       " 2: 'Leopards',\n",
       " 3: 'Motorbikes',\n",
       " 4: 'accordion',\n",
       " 5: 'airplanes',\n",
       " 6: 'anchor',\n",
       " 7: 'ant',\n",
       " 8: 'barrel',\n",
       " 9: 'bass',\n",
       " 10: 'beaver',\n",
       " 11: 'binocular',\n",
       " 12: 'bonsai',\n",
       " 13: 'brain',\n",
       " 14: 'brontosaurus',\n",
       " 15: 'buddha',\n",
       " 16: 'butterfly',\n",
       " 17: 'camera',\n",
       " 18: 'cannon',\n",
       " 19: 'car_side',\n",
       " 20: 'ceiling_fan',\n",
       " 21: 'cellphone',\n",
       " 22: 'chair',\n",
       " 23: 'chandelier',\n",
       " 24: 'cougar_body',\n",
       " 25: 'cougar_face',\n",
       " 26: 'crab',\n",
       " 27: 'crayfish',\n",
       " 28: 'crocodile',\n",
       " 29: 'crocodile_head',\n",
       " 30: 'cup',\n",
       " 31: 'dalmatian',\n",
       " 32: 'dollar_bill',\n",
       " 33: 'dolphin',\n",
       " 34: 'dragonfly',\n",
       " 35: 'electric_guitar',\n",
       " 36: 'elephant',\n",
       " 37: 'emu',\n",
       " 38: 'euphonium',\n",
       " 39: 'ewer',\n",
       " 40: 'ferry',\n",
       " 41: 'flamingo',\n",
       " 42: 'flamingo_head',\n",
       " 43: 'garfield',\n",
       " 44: 'gerenuk',\n",
       " 45: 'gramophone',\n",
       " 46: 'grand_piano',\n",
       " 47: 'hawksbill',\n",
       " 48: 'headphone',\n",
       " 49: 'hedgehog',\n",
       " 50: 'helicopter',\n",
       " 51: 'ibis',\n",
       " 52: 'inline_skate',\n",
       " 53: 'joshua_tree',\n",
       " 54: 'kangaroo',\n",
       " 55: 'ketch',\n",
       " 56: 'lamp',\n",
       " 57: 'laptop',\n",
       " 58: 'llama',\n",
       " 59: 'lobster',\n",
       " 60: 'lotus',\n",
       " 61: 'mandolin',\n",
       " 62: 'mayfly',\n",
       " 63: 'menorah',\n",
       " 64: 'metronome',\n",
       " 65: 'minaret',\n",
       " 66: 'nautilus',\n",
       " 67: 'octopus',\n",
       " 68: 'okapi',\n",
       " 69: 'pagoda',\n",
       " 70: 'panda',\n",
       " 71: 'pigeon',\n",
       " 72: 'pizza',\n",
       " 73: 'platypus',\n",
       " 74: 'pyramid',\n",
       " 75: 'revolver',\n",
       " 76: 'rhino',\n",
       " 77: 'rooster',\n",
       " 78: 'saxophone',\n",
       " 79: 'schooner',\n",
       " 80: 'scissors',\n",
       " 81: 'scorpion',\n",
       " 82: 'sea_horse',\n",
       " 83: 'snoopy',\n",
       " 84: 'soccer_ball',\n",
       " 85: 'stapler',\n",
       " 86: 'starfish',\n",
       " 87: 'stegosaurus',\n",
       " 88: 'stop_sign',\n",
       " 89: 'strawberry',\n",
       " 90: 'sunflower',\n",
       " 91: 'tick',\n",
       " 92: 'trilobite',\n",
       " 93: 'umbrella',\n",
       " 94: 'watch',\n",
       " 95: 'water_lilly',\n",
       " 96: 'wheelchair',\n",
       " 97: 'wild_cat',\n",
       " 98: 'windsor_chair',\n",
       " 99: 'wrench',\n",
       " 100: 'yin_yang'}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_to_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.io import imshow\n",
    "from skimage.transform import resize\n",
    "from skimage.feature import hog\n",
    "from skimage import exposure\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is ID:2\n",
      "label:0\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=60x60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_ID =2\n",
    "img, label = dataset[image_ID]\n",
    "print(f'This is ID:{image_ID}')\n",
    "print(f'label:{label}')\n",
    "newsize = (60, 60)\n",
    "img = img.resize(newsize)\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_caltech_101 = np.array([dataset[i][1] for i in range(len(dataset))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print(labels_caltech_101[8676])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=519x343>, 0)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "new_dataset = []\n",
    "for index, (img, label) in enumerate(dataset):\n",
    "    if isinstance(img, Image.Image):  # Check if img is a PIL Image object\n",
    "        img_path = img.filename  # Store the image path before closing it\n",
    "        img.close()  # Close the image object\n",
    "    else:\n",
    "        img_path = img  # If img is already a path\n",
    "    \n",
    "    if index % 2 == 0:  # If it's an even-indexed image\n",
    "        with Image.open(img_path) as opened_img:\n",
    "            new_dataset.append((opened_img.copy(), label))  # Append a copy of the opened image to avoid closure\n",
    "\n",
    "dataset = new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4339"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_caltech_101 = [label for index, label in enumerate(labels_caltech_101) if index % 2 == 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4339"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels_caltech_101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4339/4339 [02:50<00:00, 25.48it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from torchvision import datasets\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy.stats import skew\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize an empty list to store color moments\n",
    "color_moments_list = []\n",
    "\n",
    "# Initialize an empty list to store img_arrays (if needed)\n",
    "all_img_arrays = []\n",
    "\n",
    "# Loop through the dataset\n",
    "for image_ID in tqdm(range(len(dataset))):\n",
    "    img, label = dataset[image_ID]\n",
    "    \n",
    "    # Step 1: Resize the image to 300x100\n",
    "    new_size = (300, 100)\n",
    "    img_resized = img.resize(new_size)\n",
    "    \n",
    "    # Convert the PIL Image to a NumPy array\n",
    "    img_array = np.array(img_resized)\n",
    "    \n",
    "    #grayscale image check \n",
    "    is_gray = len(img_array.shape) == 2\n",
    "    \n",
    "    # Append the img_array to the list (if you need to keep all arrays)\n",
    "    all_img_arrays.append(img_array)\n",
    "\n",
    "    # Step 2: Partition the image into a 10x10 grid\n",
    "    for i in range(0, 300, 30):\n",
    "        for j in range(0, 100, 10):\n",
    "            grid_cell = img_array[j:j+10, i:i+30]\n",
    "            \n",
    "            # Step 3: Calculate the color moments for each grid cell\n",
    "            color_moments_dict = {}\n",
    "            for color_channel, color_name in enumerate(['Gray'] if is_gray else ['Red', 'Green', 'Blue']):\n",
    "                channel_data = grid_cell if is_gray else grid_cell[:, :, color_channel]\n",
    "\n",
    "                # Calculate mean, standard deviation, and skewness\n",
    "                channel_mean = np.mean(channel_data)\n",
    "                channel_std = np.std(channel_data)\n",
    "                if np.all(channel_data == channel_data[0]):\n",
    "                    channel_skewness = 0\n",
    "                else :\n",
    "                    channel_skewness = skew(channel_data.reshape(-1))\n",
    "                \n",
    "                # Store the color moments in the dictionary\n",
    "                color_moments_dict[f\"{color_name}_Mean\"] = channel_mean\n",
    "                color_moments_dict[f\"{color_name}_Std\"] = channel_std\n",
    "                color_moments_dict[f\"{color_name}_Skewness\"] = channel_skewness\n",
    "            \n",
    "            # Include the ImageID\n",
    "            color_moments_dict[\"ImageID\"] = image_ID\n",
    "            \n",
    "            # Append the color moments to the list\n",
    "            color_moments_list.append(color_moments_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use hog_features_dict as the feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting HOG features: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4339/4339 [00:29<00:00, 146.99it/s]\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "from skimage.feature import hog\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming you've already loaded the dataset like this:\n",
    "# dataset = datasets.Caltech101('/path/to/dataset', download=True)\n",
    "\n",
    "def extract_hog_features(image):\n",
    "    # Convert PIL Image to NumPy array\n",
    "    image_np = np.array(image)\n",
    "    \n",
    "    # Check if the image is grayscale\n",
    "    if len(image_np.shape) == 2:\n",
    "        gray_image = image_np\n",
    "    else:\n",
    "        gray_image = rgb2gray(image_np)\n",
    "    \n",
    "    # Resize the image\n",
    "    resized_image = resize(gray_image, (300, 100))\n",
    "    \n",
    "    # Compute the HOG features\n",
    "    features, hog_image = hog(resized_image, orientations=9, pixels_per_cell=(30, 10),\n",
    "                              cells_per_block=(1, 1), visualize=True)\n",
    "    \n",
    "    return features, hog_image\n",
    "\n",
    "# Initialize empty dictionaries to store features and labels\n",
    "hog_features_dict = {}\n",
    "hog_images_dict = {}\n",
    "labels_dict = {}\n",
    "\n",
    "# Loop over the dataset with progress bar\n",
    "for i in tqdm(range(len(dataset)), desc=\"Extracting HOG features\"):\n",
    "    image, label = dataset[i]\n",
    "    features, hog_image = extract_hog_features(image)\n",
    "    hog_features_dict[i] = features\n",
    "    hog_images_dict[i] = hog_image\n",
    "    labels_dict[i] = label\n",
    "\n",
    "# Now:\n",
    "# hog_features_dict contains the 900-dimensional feature descriptor for each image, indexed by dataset index\n",
    "# hog_images_dict contains the visual representation of the HOG features for each image, indexed by dataset index\n",
    "# labels_dict contains the label for each image, indexed by dataset index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4339\n"
     ]
    }
   ],
   "source": [
    "print(len(hog_features_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "433900\n"
     ]
    }
   ],
   "source": [
    "print(len(color_moments_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resnet computations \n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "    transforms.Normalize([0.53994344, 0.52009986, 0.49254049], \n",
    "                         [0.31415099, 0.30712622, 0.31878401]),  # Normalize the images\n",
    "])\n",
    "\n",
    "def resnet_computations(hook_layer, dataset):\n",
    "    \n",
    "    # List to store the output tensors for each image along with their ImageID\n",
    "    outputs_with_ids = []\n",
    "    skipped_images = []\n",
    "    \n",
    "    # List to temporarily capture the output tensor from the hook\n",
    "    captured_output = [None]\n",
    "\n",
    "    # Hook function to capture the output tensor of a specified layer\n",
    "    def capture_output(module, input, output):\n",
    "        captured_output[0] = output\n",
    "\n",
    "    # Register the hook function to the specified layer\n",
    "    if hook_layer == 'avgpool':\n",
    "        hook = resnet_model.avgpool.register_forward_hook(capture_output)\n",
    "    elif hook_layer == 'layer3':\n",
    "        hook = resnet_model.layer3.register_forward_hook(capture_output)\n",
    "    elif hook_layer == 'fc':   \n",
    "        hook = resnet_model.fc.register_forward_hook(capture_output)\n",
    "\n",
    "    # Loop through the dataset\n",
    "    for i in tqdm(range(len(dataset))):\n",
    "        try:\n",
    "            img, label = dataset[i]\n",
    "            #skipping grayscale images \n",
    "            if img.mode == 'L' or img.mode == '1':\n",
    "                img = img.convert(\"RGB\")\n",
    "\n",
    "            # Apply transformations and prepare image batch\n",
    "            img_tensor = transform(img)\n",
    "            img_batch = img_tensor.unsqueeze(0)  # Add a batch dimension\n",
    "\n",
    "            # Forward pass (disable gradient computation to save memory)\n",
    "            with torch.no_grad():\n",
    "                resnet_model(img_batch)\n",
    "            \n",
    "            # Retrieve the captured output tensor\n",
    "            resnet_output = captured_output[0]\n",
    "            if resnet_output is None:\n",
    "                print(\"Warning: Hook Not Triggered\")\n",
    "                skipped_images.append(i)\n",
    "                continue\n",
    "\n",
    "            # Process the output tensor depending on the specified layer and store it in a dictionary\n",
    "            output_dict = {\"ImageID\": i}\n",
    "            if hook_layer == 'avgpool':\n",
    "                avgpool_output = resnet_output.flatten().cpu().numpy()\n",
    "                averaged_values = [(avgpool_output[i] + avgpool_output[i+1]) / 2.0 for i in range(0, len(avgpool_output), 2)]\n",
    "                output_dict[\"Output\"] = np.array(averaged_values)\n",
    "            elif hook_layer == 'layer3':\n",
    "                avg_vector = resnet_output.mean(dim=[2, 3]).cpu().numpy().squeeze()\n",
    "                output_dict[\"Output\"] = avg_vector\n",
    "            elif hook_layer == 'fc':\n",
    "                output_dict[\"Output\"] = resnet_output.cpu().numpy().squeeze()\n",
    "            \n",
    "            # Append the dictionary to the list\n",
    "            outputs_with_ids.append(output_dict)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing ImageID {i}: {e}\")\n",
    "            skipped_images.append(i)\n",
    "    # Remove the hook to free resources\n",
    "    hook.remove()\n",
    "    \n",
    "    return outputs_with_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4339/4339 [02:32<00:00, 28.46it/s]\n"
     ]
    }
   ],
   "source": [
    "output_avgpool_with_ids = resnet_computations('avgpool',dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4339\n"
     ]
    }
   ],
   "source": [
    "print(len(output_avgpool_with_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4339/4339 [02:27<00:00, 29.32it/s]\n"
     ]
    }
   ],
   "source": [
    "output_layers3_with_ids = resnet_computations('layer3',dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4339\n"
     ]
    }
   ],
   "source": [
    "print(len(output_layers3_with_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4339/4339 [02:42<00:00, 26.67it/s]\n"
     ]
    }
   ],
   "source": [
    "output_fc_with_ids = resnet_computations('fc',dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4339\n"
     ]
    }
   ],
   "source": [
    "print(len(output_fc_with_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD, NMF\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd \n",
    "# df_color_moments = pd.read_csv(\"color_moments_with_imageID.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(df_color_moments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_columns = df_color_moments.columns[1:-1]\n",
    "# data_color_moments = df_color_moments[feature_columns].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data_color_moments[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 5 \n",
    "# svd = TruncatedSVD(n_components = k)\n",
    "# latent_semantics = svd.fit_transform(data_color_moments)\n",
    "# print(\"Reduced Data:(Latent Semantics)\")\n",
    "# print(latent_semantics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVD_calc(feature_matrix,k):\n",
    "    #print(\"Enter SVD calc\")\n",
    "    svd = TruncatedSVD(n_components=k)\n",
    "    latent_semantics = svd.fit_transform(feature_matrix)\n",
    "    #print(\"The latent semantics are:\")\n",
    "    print(latent_semantics)\n",
    "    return latent_semantics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NMF_calculator(feature_matrix,k):\n",
    "    nmf = NMF(n_components=k)\n",
    "    W = nmf.fit_transform(feature_matrix)\n",
    "    H = nmf.components_\n",
    "    return H "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LDA_calculator(feature_matrix,k):\n",
    "    lda = LinearDiscriminantAnalysis(n_components = k)\n",
    "    lda_result = lda.fit_transform(feature_matrix,labels_caltech_101)\n",
    "    top_k_latent = lda.scalings_[:, :k]\n",
    "    return top_k_latent\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means_calculator(feature_matrix,k):\n",
    "    kmeans = KMeans(n_clusters = k,random_state=0)\n",
    "    kmeans.fit(feature_matrix)\n",
    "    top_k_latent_semantics = kmeans.cluster_centers_\n",
    "    return top_k_latent_semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below Is the Code Of Block of converting the Phase 1 results into feature matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd \n",
    "# df_color_moments = pd.read_csv(\"color_moments_with_imageID.csv\")\n",
    "# feature_columns = df_color_moments.columns[1:-1]\n",
    "# data_color_moments = df_color_moments[feature_columns].values\n",
    "# print(feature_columns)\n",
    "# print(data_color_moments[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the Below code block use data_color_moments as the feature matrix for SVD \n",
    "#### Use the X_color_moments for NMF and X_standardized_color_moments for remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd \n",
    "color_moments_df = pd.DataFrame(color_moments_list)\n",
    "color_moments_df.fillna(color_moments_df.mean(), inplace=True)\n",
    "data_color_moments= color_moments_df.drop(columns=\"ImageID\").to_numpy()\n",
    "n_grids_per_image = 10 * 10  # 10x10 grid for each image\n",
    "n_features_per_grid = data_color_moments.shape[1]\n",
    "X_color_moments = data_color_moments.reshape(len(dataset), n_grids_per_image * n_features_per_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_2 = StandardScaler()\n",
    "X_standardized_color_moments = scaler_2.fit_transform(X_color_moments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X_standardized_color_moments[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_color_moments))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use X_hog for nmf and X_hog_standardized for remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_hog = np.array(list(hog_features_dict.values()))\n",
    "scaler = StandardScaler()\n",
    "X_hog_standardized = scaler.fit_transform(X_hog)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X_hog_standardized[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Matrix Conversion of Resnet computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_resnet_avgpool = np.array([entry[\"Output\"] for entry in output_avgpool_with_ids])\n",
    "X_resnet_layers3 = np.array([entry[\"Output\"] for entry in output_layers3_with_ids])\n",
    "X_resnet_fc = np.array([entry[\"Output\"] for entry in output_fc_with_ids])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_resnet_layers3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use X_resnet_{layer_value} for NMF and X_standardized_resnet_{layer_value} for remaining reduction technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardizing the data \n",
    "scaler_1 = StandardScaler()\n",
    "X_standardized_resnet_avgpool = scaler_1.fit_transform(X_resnet_avgpool)\n",
    "X_standardized_resnet_layers3 = scaler_1.fit_transform(X_resnet_layers3)\n",
    "X_standardized_resnet_fc = scaler_1.fit_transform(X_resnet_fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_standardized_resnet_fc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X_standardized_resnet_fc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Create the UI here and call appropriate functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_model_data(feature_model):\n",
    "    if feature_model == \"Color Moments\":\n",
    "        X_data = X_color_moments\n",
    "        X_standardized = X_standardized_color_moments\n",
    "    elif feature_model == \"HOG Descriptor\":\n",
    "        X_data = X_hog\n",
    "        X_standardized = X_hog_standardized\n",
    "    elif  feature_model == \"Resnet FC\":\n",
    "        X_data = X_resnet_fc\n",
    "        X_standardized = X_standardized_resnet_fc\n",
    "    elif feature_model == \"Resnet Avgpool\":\n",
    "        X_data = X_resnet_avgpool\n",
    "        X_standardized= X_standardized_resnet_avgpool \n",
    "    elif feature_model == \"Resnet Layer 3\":\n",
    "        X_data = X_resnet_layers3\n",
    "        X_standardized= X_standardized_resnet_layers3\n",
    "    else:\n",
    "        return -1\n",
    "    return X_data,X_standardized\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dimensionality_reduction(X_data,X_standardized,technique,k):\n",
    "    if technique == 'SVD':\n",
    "        #print(\"Entered SVD\")\n",
    "        result = SVD_calc(X_standardized,k)\n",
    "    elif technique == 'NNMF':\n",
    "        result = NMF_calculator(X_data,k)\n",
    "    elif technique =='LDA':\n",
    "        result = LDA_calculator(X_standardized,k)\n",
    "    elif technique == 'k-means':\n",
    "        result = k_means_calculator(X_standardized,k)\n",
    "    else :\n",
    "        return -1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    #Select a Feature Model\n",
    "    # Prompt user for feature model\n",
    "    print(\"Choose a feature model:\")\n",
    "    print(\"1. Color Moments\")\n",
    "    print(\"2. HOG Descriptor\")\n",
    "    print(\"3. Resnet FC\")\n",
    "    print(\"4. Resnet Avgpool\")\n",
    "    print(\"5. Resnet Layer 3\")\n",
    "    feature_model_choice = input(\"Enter your choice(number): \")\n",
    "    # Convert choice to string name\n",
    "    feature_model = [\"Color Moments\", \"HOG Descriptor\", \"Resnet FC\", \"Resnet Avgpool\", \"Resnet Layer 3\"][int(feature_model_choice) - 1]\n",
    "    #print(feature_model)\n",
    "    X_data, X_standardized_data = get_feature_model_data(feature_model)\n",
    "#Select the Dimensionality Reduction Technique \n",
    "    k = int(input('Please Enter the value of k'))\n",
    "    # Prompt user for dimensionality reduction technique\n",
    "    print(\"\\nChoose a dimensionality reduction technique:\")\n",
    "    print(\"1. SVD\")\n",
    "    print(\"2. NNMF\")\n",
    "    print(\"3. LDA\")\n",
    "    print(\"4. k-means\")\n",
    "    technique_choice = input(\"Enter your choice: \")\n",
    "    #Convert choice to specified string \n",
    "    technique = [\"SVD\", \"NNMF\", \"LDA\", \"k-means\"][int(technique_choice) - 1]\n",
    "    result = dimensionality_reduction(X_data,X_standardized_data,technique,k,)\n",
    "    print(f\"top {k} latent-semantics of {feature_model} using {technique}\")\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(labels_caltech_101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(hog_features_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hog_features_dict[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(color_moments_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_moments_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(output_avgpool_with_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(output_avgpool_with_ids[435]['Output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(output_layers3_with_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(output_fc_with_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HOG TENSOR CREATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_empty_tensor(num_images, feature_length, num_labels):\n",
    "    \"\"\"\n",
    "    Creates an empty tensor filled with zeros.\n",
    "    \"\"\"\n",
    "    return np.zeros((num_images, feature_length, num_labels))\n",
    "\n",
    "def get_label_index(label, all_labels):\n",
    "    \"\"\"\n",
    "    Returns the index of the given label in the all_labels list.\n",
    "    \"\"\"\n",
    "    return all_labels.index(label)\n",
    "\n",
    "def construct_tensor(feature_vectors, labels_list, all_labels):\n",
    "    num_images = len(feature_vectors)\n",
    "    feature_length = len(next(iter(feature_vectors.values())))  # Length of a feature vector\n",
    "    num_labels = len(all_labels)\n",
    "\n",
    "    tensor = create_empty_tensor(num_images, feature_length, num_labels)\n",
    "\n",
    "    for idx, feature in enumerate(feature_vectors.values()):\n",
    "        # Set the feature vector in the tensor\n",
    "        tensor[idx, :, :] = np.array(feature)[:, np.newaxis]\n",
    "\n",
    "        # Set the label in the tensor\n",
    "        label = labels_list[idx]\n",
    "        label_index = get_label_index(label, all_labels)\n",
    "        tensor[idx, :, label_index] = 1\n",
    "\n",
    "    return tensor\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "hog_tensor = construct_tensor(hog_features_dict, labels_caltech_101, list(set(labels_caltech_101)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"hog_tensor.pkl\", \"wb\") as f:\n",
    "    pickle.dump(hog_tensor, f, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4339, 900, 101)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hog_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COLOR MOMENTS TENSOR CREATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_color_moments_tensor(data_list):\n",
    "    num_images = len(data_list) // 100\n",
    "    features_per_element = 9 # Max 9 for RGB\n",
    "    elements_per_image = 100\n",
    "\n",
    "    tensor = np.zeros((num_images, features_per_element * elements_per_image, len(set(labels_caltech_101))))\n",
    "\n",
    "    for i in range(num_images):\n",
    "        for j in range(elements_per_image):\n",
    "            element = data_list[i*100 + j]\n",
    "            if 'Red_Mean' in element:  # RGB features\n",
    "                features = [element[key] for key in ['Red_Mean', 'Red_Std', 'Red_Skewness',\n",
    "                                                     'Green_Mean', 'Green_Std', 'Green_Skewness',\n",
    "                                                     'Blue_Mean', 'Blue_Std', 'Blue_Skewness']]\n",
    "            else:  # Grayscale features\n",
    "                features = [element[key] for key in ['Gray_Mean', 'Gray_Std', 'Gray_Skewness']]\n",
    "                # Padding to match the RGB feature length\n",
    "                features.extend([0] * (9 - len(features)))\n",
    "            \n",
    "            start_idx = j * features_per_element\n",
    "            end_idx = start_idx + features_per_element\n",
    "            tensor[i, start_idx:end_idx, j] = features\n",
    "\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4339, 900, 101)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "color_moments_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_moments_tensor = construct_color_moments_tensor(color_moments_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"color_moments_tensor.pkl\", \"wb\") as f:\n",
    "    pickle.dump(color_moments_tensor, f, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a tensor\n",
    "\n",
    "index_to_extract = 0\n",
    "extracted_tensor = color_moments_tensor[index_to_extract, :, :]\n",
    "\n",
    "# Printing the entire 2D tensor without truncation by iterating through its rows and columns\n",
    "output_data = []\n",
    "for row in extracted_tensor:\n",
    "    output_data.append(list(row))\n",
    "\n",
    "# output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RESNET TENSOR CREATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4339"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels_caltech_101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_resnet_tensor(data_list):\n",
    "    num_images = len(data_list)\n",
    "    feature_length = len(data_list[0]['Output'])\n",
    "    \n",
    "    tensor = np.zeros((num_images, feature_length, len(set(labels_caltech_101))))\n",
    "\n",
    "    for i, element in enumerate(data_list):\n",
    "        features = element['Output']\n",
    "        label_index = labels_caltech_101[i]\n",
    "        tensor[i, :, label_index] = features\n",
    "\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_avgpool_tensor = construct_resnet_tensor(output_avgpool_with_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4339, 1024, 101)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_avgpool_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"resnet_avgpool_tensor.pkl\", \"wb\") as f:\n",
    "    pickle.dump(resnet_avgpool_tensor, f, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_layer3_tensor = construct_resnet_tensor(output_layers3_with_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4339, 1024, 101)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_layer3_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresnet_layer3_tensor.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mpickle\u001b[49m\u001b[38;5;241m.\u001b[39mdump(resnet_layer3_tensor, f, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "with open(\"resnet_layer3_tensor.pkl\", \"wb\") as f:\n",
    "    pickle.dump(resnet_layer3_tensor, f, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_fc_tensor = construct_resnet_tensor(output_fc_with_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4339, 1000, 101)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_fc_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"resnet_fc_tensor.pkl\", \"wb\") as f:\n",
    "    pickle.dump(resnet_fc_tensor, f, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main function for CP decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorly as tl\n",
    "from tensorly.decomposition import parafac\n",
    "\n",
    "feature_tensor = {\n",
    "    1: hog_tensor,\n",
    "    2: color_moments_tensor,\n",
    "    3: resnet_avgpool_tensor,\n",
    "    4: resnet_layer3_tensor,\n",
    "    5: resnet_fc_tensor\n",
    "}\n",
    "\n",
    "# 1: HOG\n",
    "# 2: Color Moments\n",
    "# 3: Resnet Avgpool\n",
    "# 4: Resnet Layer3\n",
    "# 5: Resnet FC\n",
    "\n",
    "def extract_latent_semantics(tensor, k, labels):\n",
    "    # Perform CP-decomposition\n",
    "    weights, factors = parafac(tensor, rank=k)\n",
    "    \n",
    "    # Extract the weights for the label mode (assuming it's the last mode)\n",
    "    label_weights = factors[2]\n",
    "    \n",
    "    # Sort the weights and associate with labels\n",
    "    sorted_indices = label_weights.argsort(axis=0)[::-1].flatten()\n",
    "    sorted_labels = [label_to_name[labels[i]] for i in sorted_indices]\n",
    "    sorted_weights = label_weights[sorted_indices].flatten()\n",
    "    \n",
    "    # Prepare the label-weight pairs\n",
    "    label_weight_pairs = list(zip(sorted_labels, sorted_weights))\n",
    "    \n",
    "    return label_weight_pairs\n",
    "\n",
    "def save_to_file(label_weight_pairs, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        for label, weight in label_weight_pairs:\n",
    "            f.write(f\"{label}: {weight}\\n\")\n",
    "\n",
    "def main_task_4(feature_model_id, k):\n",
    "    tensor = feature_tensor[feature_model_id]\n",
    "    labels = list(set(labels_caltech_101))\n",
    "    \n",
    "    latent_semantics = extract_latent_semantics(tensor, k, labels)\n",
    "    save_to_file(latent_semantics, \"latent_semantics_output.txt\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_task_4(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import TruncatedSVD, NMF, LatentDirichletAllocation\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "\n",
    "def create_label_similarity_matrix(tensor):\n",
    "    # Average across the image dimension to get label representations\n",
    "    label_representations = np.mean(tensor, axis=0).T\n",
    "    # Compute cosine similarities for the 900-dimensional representations\n",
    "    similarities = cosine_similarity(label_representations)\n",
    " \n",
    "    return similarities\n",
    "\n",
    "\n",
    "def dimensionality_reduction(matrix, method, k):\n",
    "    if method == \"SVD\":\n",
    "        svd = TruncatedSVD(n_components=k)\n",
    "        reduced_matrix = svd.fit_transform(matrix)\n",
    "    elif method == \"NNMF\":\n",
    "        nmf = NMF(n_components=k)\n",
    "        reduced_matrix = nmf.fit_transform(matrix)\n",
    "    elif method == \"LDA\":\n",
    "        lda = LatentDirichletAllocation(n_components=k)\n",
    "        reduced_matrix = lda.fit_transform(matrix)\n",
    "    elif method == \"k-means\":\n",
    "        kmeans = KMeans(n_clusters=k)\n",
    "        reduced_matrix = kmeans.fit_transform(matrix)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "    return reduced_matrix\n",
    "\n",
    "def save_to_file(matrix, filename):\n",
    "    np.savetxt(filename, matrix)\n",
    "\n",
    "def main_task_5(feature_model_id, k, reduction_method):\n",
    "    tensor = feature_tensor[feature_model_id]\n",
    "    similarity_matrix = create_label_similarity_matrix(tensor)\n",
    "    reduced_matrix = dimensionality_reduction(similarity_matrix, reduction_method, k)\n",
    "    \n",
    "    # Save latent semantics\n",
    "    # filename = f\"latent_semantics_{reduction_method}.txt\"\n",
    "    # save_to_file(reduced_matrix, filename)\n",
    "    \n",
    "    # List label-weight pairs\n",
    "    reduced_matrix = pd.DataFrame(reduced_matrix)\n",
    "    reduced_matrix.index = reduced_matrix.index.map(label_to_name)\n",
    "    print(pd.DataFrame(reduced_matrix))\n",
    "    reduced_matrix.to_csv(f'latent_semantics_{reduction_method}.csv')\n",
    "    # Compute weights for each label\n",
    "    label_weights = np.sum(reduced_matrix, axis=1)\n",
    "\n",
    "    print(label_weights)\n",
    "    # Sort labels based on their weights in descending order\n",
    "    sorted_indices = label_weights.argsort()[::-1]\n",
    "    sorted_indices_names = [label_to_name[i] for i in label_weights.argsort()[::-1]]\n",
    "    sorted_weights = label_weights[sorted_indices]\n",
    "    \n",
    "    # Pair the labels with their weights\n",
    "    label_weight_pairs = list(zip(sorted_indices_names, sorted_weights))\n",
    "    return label_weight_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      0         1         2         3\n",
      "Faces          0.608837  0.000000  0.174331  0.260555\n",
      "Faces_easy     0.575990  0.000000  0.194487  0.280873\n",
      "Leopards       0.122503  0.031747  0.493672  0.570834\n",
      "Motorbikes     0.122917  0.030626  0.493833  0.569942\n",
      "accordion      0.078635  0.401943  0.377897  0.374183\n",
      "...                 ...       ...       ...       ...\n",
      "wheelchair     0.122500  0.031467  0.495575  0.572130\n",
      "wild_cat       0.050852  0.281626  0.443387  0.470185\n",
      "windsor_chair  0.122725  0.030909  0.494682  0.570506\n",
      "wrench         0.141745  0.056886  0.472583  0.538265\n",
      "yin_yang       0.762748  0.004963  0.071277  0.148804\n",
      "\n",
      "[101 rows x 4 columns]\n",
      "Faces            1.043722\n",
      "Faces_easy       1.051350\n",
      "Leopards         1.218756\n",
      "Motorbikes       1.217319\n",
      "accordion        1.232658\n",
      "                   ...   \n",
      "wheelchair       1.221673\n",
      "wild_cat         1.246050\n",
      "windsor_chair    1.218822\n",
      "wrench           1.209479\n",
      "yin_yang         0.987792\n",
      "Length: 101, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/06/2kgvw4c965v4sfv4w1ythkgc0000gn/T/ipykernel_31191/2111718228.py:57: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  sorted_weights = label_weights[sorted_indices]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('ant', 1.266302666457901),\n",
       " ('mayfly', 1.2660540645559129),\n",
       " ('chandelier', 1.2659276250123948),\n",
       " ('starfish', 1.2658113491822236),\n",
       " ('sea_horse', 1.265684846063221),\n",
       " ('bonsai', 1.265130869827797),\n",
       " ('crocodile_head', 1.2648987139912613),\n",
       " ('crocodile', 1.2647054074127806),\n",
       " ('pyramid', 1.2644256226390413),\n",
       " ('ketch', 1.2643486252569538),\n",
       " ('schooner', 1.2639246840223515),\n",
       " ('ferry', 1.2638100622007418),\n",
       " ('minaret', 1.262315042729306),\n",
       " ('beaver', 1.2465719570739835),\n",
       " ('platypus', 1.2465352968410786),\n",
       " ('rooster', 1.246255013220484),\n",
       " ('elephant', 1.2461566474141705),\n",
       " ('wild_cat', 1.2460498081459912),\n",
       " ('dalmatian', 1.2459348304683149),\n",
       " ('hawksbill', 1.2458557941809048),\n",
       " ('panda', 1.245734531199959),\n",
       " ('lotus', 1.2455846082155932),\n",
       " ('flamingo_head', 1.2451856359444595),\n",
       " ('cougar_face', 1.2450173308554113),\n",
       " ('water_lilly', 1.2444127261991804),\n",
       " ('hedgehog', 1.244328487108866),\n",
       " ('sunflower', 1.242073257215002),\n",
       " ('trilobite', 1.2352331013946394),\n",
       " ('pizza', 1.234509061370035),\n",
       " ('accordion', 1.2326575430884663),\n",
       " ('butterfly', 1.2286462270857639),\n",
       " ('pagoda', 1.2278102631725587),\n",
       " ('crab', 1.2273729880930382),\n",
       " ('bass', 1.2266344357197965),\n",
       " ('dolphin', 1.2255008671596013),\n",
       " ('scorpion', 1.2231617720161303),\n",
       " ('crayfish', 1.2231131227384497),\n",
       " ('pigeon', 1.2224857022128601),\n",
       " ('cougar_body', 1.2224212373125614),\n",
       " ('cannon', 1.222420209119179),\n",
       " ('helicopter', 1.2223205673112423),\n",
       " ('brontosaurus', 1.2222504791517337),\n",
       " ('flamingo', 1.2220746308909174),\n",
       " ('stegosaurus', 1.22202924226655),\n",
       " ('okapi', 1.2219025587073793),\n",
       " ('kangaroo', 1.2218990676192436),\n",
       " ('ceiling_fan', 1.2217532521601662),\n",
       " ('chair', 1.2216813405002536),\n",
       " ('wheelchair', 1.2216730797041917),\n",
       " ('llama', 1.2216509901368577),\n",
       " ('ibis', 1.2216118834713143),\n",
       " ('gramophone', 1.2215285641087288),\n",
       " ('tick', 1.2214256952771736),\n",
       " ('rhino', 1.2214231669991888),\n",
       " ('airplanes', 1.221391878558462),\n",
       " ('gerenuk', 1.2212841568338093),\n",
       " ('emu', 1.2210842161730957),\n",
       " ('snoopy', 1.2201457412490033),\n",
       " ('menorah', 1.2189405193137284),\n",
       " ('windsor_chair', 1.2188220259870515),\n",
       " ('Leopards', 1.2187558867484127),\n",
       " ('car_side', 1.2186635047918688),\n",
       " ('joshua_tree', 1.2186198764681613),\n",
       " ('garfield', 1.21746166369677),\n",
       " ('Motorbikes', 1.2173187760496647),\n",
       " ('scissors', 1.2161786571984416),\n",
       " ('stapler', 1.2094903668439505),\n",
       " ('wrench', 1.2094790325042633),\n",
       " ('inline_skate', 1.208558016023749),\n",
       " ('stop_sign', 1.2079270552807564),\n",
       " ('revolver', 1.2075333649449107),\n",
       " ('grand_piano', 1.20693093795441),\n",
       " ('laptop', 1.206906935928711),\n",
       " ('metronome', 1.2066185671962302),\n",
       " ('umbrella', 1.2059241173967914),\n",
       " ('mandolin', 1.2046608667917935),\n",
       " ('dragonfly', 1.2028392088699673),\n",
       " ('lamp', 1.2022545696764158),\n",
       " ('electric_guitar', 1.2014662876465116),\n",
       " ('saxophone', 1.2010296672574776),\n",
       " ('anchor', 1.2007028727119535),\n",
       " ('dollar_bill', 1.1973158527186225),\n",
       " ('Faces_easy', 1.051349718982563),\n",
       " ('headphone', 1.0473960411073946),\n",
       " ('cup', 1.047295313596861),\n",
       " ('ewer', 1.0469178335663853),\n",
       " ('Faces', 1.043721731905698),\n",
       " ('octopus', 1.0390450021810114),\n",
       " ('buddha', 1.037845536073894),\n",
       " ('yin_yang', 0.9877924454820932),\n",
       " ('soccer_ball', 0.9829055704125416),\n",
       " ('camera', 0.9819811480972926),\n",
       " ('nautilus', 0.9817310582278931),\n",
       " ('watch', 0.9786810518688689),\n",
       " ('brain', 0.9766816265641476),\n",
       " ('strawberry', 0.9746395631784694),\n",
       " ('euphonium', 0.9730343446735379),\n",
       " ('cellphone', 0.9706901199716846),\n",
       " ('binocular', 0.9697085036703702),\n",
       " ('barrel', 0.9680033357239864),\n",
       " ('lobster', 0.9667923582452876)]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_task_5(4, 4, \"NNMF\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
