{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np \n",
    "import torch \n",
    "from torchvision import transforms, models \n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_model = models.resnet50(weights = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.8.1\n"
     ]
    }
   ],
   "source": [
    "print(cv2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "131740031it [00:08, 16193324.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/abhinav/Desktop/CSE515-Project/caltech101/101_ObjectCategories.tar.gz to /Users/abhinav/Desktop/CSE515-Project/caltech101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14028800it [00:00, 16346789.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/abhinav/Desktop/CSE515-Project/caltech101/Annotations.tar to /Users/abhinav/Desktop/CSE515-Project/caltech101\n",
      "8677\n"
     ]
    }
   ],
   "source": [
    "dataset = torchvision.datasets.Caltech101('/Users/abhinav/Desktop/CSE515-Project', download=True)\n",
    "data_loader = torch.utils.data.DataLoader(dataset,\n",
    "batch_size=4,\n",
    "shuffle=True,\n",
    "num_workers=8)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.io import imshow\n",
    "from skimage.transform import resize\n",
    "from skimage.feature import hog\n",
    "from skimage import exposure\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is ID:8000\n",
      "label:92\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=60x60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_ID =8000\n",
    "img, label = dataset[image_ID]\n",
    "print(f'This is ID:{image_ID}')\n",
    "print(f'label:{label}')\n",
    "newsize = (60, 60)\n",
    "img = img.resize(newsize)\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_caltech_101 = np.array([dataset[i][1] for i in range(len(dataset))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print(labels_caltech_101[8676])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8677/8677 [05:56<00:00, 24.31it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from torchvision import datasets\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy.stats import skew\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize an empty list to store color moments\n",
    "color_moments_list = []\n",
    "\n",
    "# Initialize an empty list to store img_arrays (if needed)\n",
    "all_img_arrays = []\n",
    "\n",
    "# Loop through the dataset\n",
    "for image_ID in tqdm(range(len(dataset))):\n",
    "    img, label = dataset[image_ID]\n",
    "    \n",
    "    # Step 1: Resize the image to 300x100\n",
    "    new_size = (300, 100)\n",
    "    img_resized = img.resize(new_size)\n",
    "    \n",
    "    # Convert the PIL Image to a NumPy array\n",
    "    img_array = np.array(img_resized)\n",
    "    \n",
    "    #grayscale image check \n",
    "    is_gray = len(img_array.shape) == 2\n",
    "    \n",
    "    # Append the img_array to the list (if you need to keep all arrays)\n",
    "    all_img_arrays.append(img_array)\n",
    "\n",
    "    # Step 2: Partition the image into a 10x10 grid\n",
    "    for i in range(0, 300, 30):\n",
    "        for j in range(0, 100, 10):\n",
    "            grid_cell = img_array[j:j+10, i:i+30]\n",
    "            \n",
    "            # Step 3: Calculate the color moments for each grid cell\n",
    "            color_moments_dict = {}\n",
    "            for color_channel, color_name in enumerate(['Gray'] if is_gray else ['Red', 'Green', 'Blue']):\n",
    "                channel_data = grid_cell if is_gray else grid_cell[:, :, color_channel]\n",
    "\n",
    "                # Calculate mean, standard deviation, and skewness\n",
    "                channel_mean = np.mean(channel_data)\n",
    "                channel_std = np.std(channel_data)\n",
    "                if np.all(channel_data == channel_data[0]):\n",
    "                    channel_skewness = 0\n",
    "                else :\n",
    "                    channel_skewness = skew(channel_data.reshape(-1))\n",
    "                \n",
    "                # Store the color moments in the dictionary\n",
    "                color_moments_dict[f\"{color_name}_Mean\"] = channel_mean\n",
    "                color_moments_dict[f\"{color_name}_Std\"] = channel_std\n",
    "                color_moments_dict[f\"{color_name}_Skewness\"] = channel_skewness\n",
    "            \n",
    "            # Include the ImageID\n",
    "            color_moments_dict[\"ImageID\"] = image_ID\n",
    "            \n",
    "            # Append the color moments to the list\n",
    "            color_moments_list.append(color_moments_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use hog_features_dict as the feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting HOG features: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8677/8677 [01:04<00:00, 133.71it/s]\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "from skimage.feature import hog\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming you've already loaded the dataset like this:\n",
    "# dataset = datasets.Caltech101('/path/to/dataset', download=True)\n",
    "\n",
    "def extract_hog_features(image):\n",
    "    # Convert PIL Image to NumPy array\n",
    "    image_np = np.array(image)\n",
    "    \n",
    "    # Check if the image is grayscale\n",
    "    if len(image_np.shape) == 2:\n",
    "        gray_image = image_np\n",
    "    else:\n",
    "        gray_image = rgb2gray(image_np)\n",
    "    \n",
    "    # Resize the image\n",
    "    resized_image = resize(gray_image, (300, 100))\n",
    "    \n",
    "    # Compute the HOG features\n",
    "    features, hog_image = hog(resized_image, orientations=9, pixels_per_cell=(30, 10),\n",
    "                              cells_per_block=(1, 1), visualize=True)\n",
    "    \n",
    "    return features, hog_image\n",
    "\n",
    "# Initialize empty dictionaries to store features and labels\n",
    "hog_features_dict = {}\n",
    "hog_images_dict = {}\n",
    "labels_dict = {}\n",
    "\n",
    "# Loop over the dataset with progress bar\n",
    "for i in tqdm(range(len(dataset)), desc=\"Extracting HOG features\"):\n",
    "    image, label = dataset[i]\n",
    "    features, hog_image = extract_hog_features(image)\n",
    "    hog_features_dict[i] = features\n",
    "    hog_images_dict[i] = hog_image\n",
    "    labels_dict[i] = label\n",
    "\n",
    "# Now:\n",
    "# hog_features_dict contains the 900-dimensional feature descriptor for each image, indexed by dataset index\n",
    "# hog_images_dict contains the visual representation of the HOG features for each image, indexed by dataset index\n",
    "# labels_dict contains the label for each image, indexed by dataset index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8677\n"
     ]
    }
   ],
   "source": [
    "print(len(hog_features_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "867700\n"
     ]
    }
   ],
   "source": [
    "print(len(color_moments_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resnet computations \n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "    transforms.Normalize([0.53994344, 0.52009986, 0.49254049], \n",
    "                         [0.31415099, 0.30712622, 0.31878401]),  # Normalize the images\n",
    "])\n",
    "\n",
    "def resnet_computations(hook_layer, dataset):\n",
    "    \n",
    "    # List to store the output tensors for each image along with their ImageID\n",
    "    outputs_with_ids = []\n",
    "    skipped_images = []\n",
    "    \n",
    "    # List to temporarily capture the output tensor from the hook\n",
    "    captured_output = [None]\n",
    "\n",
    "    # Hook function to capture the output tensor of a specified layer\n",
    "    def capture_output(module, input, output):\n",
    "        captured_output[0] = output\n",
    "\n",
    "    # Register the hook function to the specified layer\n",
    "    if hook_layer == 'avgpool':\n",
    "        hook = resnet_model.avgpool.register_forward_hook(capture_output)\n",
    "    elif hook_layer == 'layer3':\n",
    "        hook = resnet_model.layer3.register_forward_hook(capture_output)\n",
    "    elif hook_layer == 'fc':   \n",
    "        hook = resnet_model.fc.register_forward_hook(capture_output)\n",
    "\n",
    "    # Loop through the dataset\n",
    "    for i in tqdm(range(len(dataset))):\n",
    "        try:\n",
    "            img, label = dataset[i]\n",
    "            #skipping grayscale images \n",
    "            if img.mode == 'L' or img.mode == '1':\n",
    "                img = img.convert(\"RGB\")\n",
    "\n",
    "            # Apply transformations and prepare image batch\n",
    "            img_tensor = transform(img)\n",
    "            img_batch = img_tensor.unsqueeze(0)  # Add a batch dimension\n",
    "\n",
    "            # Forward pass (disable gradient computation to save memory)\n",
    "            with torch.no_grad():\n",
    "                resnet_model(img_batch)\n",
    "            \n",
    "            # Retrieve the captured output tensor\n",
    "            resnet_output = captured_output[0]\n",
    "            if resnet_output is None:\n",
    "                print(\"Warning: Hook Not Triggered\")\n",
    "                skipped_images.append(i)\n",
    "                continue\n",
    "\n",
    "            # Process the output tensor depending on the specified layer and store it in a dictionary\n",
    "            output_dict = {\"ImageID\": i}\n",
    "            if hook_layer == 'avgpool':\n",
    "                avgpool_output = resnet_output.flatten().cpu().numpy()\n",
    "                averaged_values = [(avgpool_output[i] + avgpool_output[i+1]) / 2.0 for i in range(0, len(avgpool_output), 2)]\n",
    "                output_dict[\"Output\"] = np.array(averaged_values)\n",
    "            elif hook_layer == 'layer3':\n",
    "                avg_vector = resnet_output.mean(dim=[2, 3]).cpu().numpy().squeeze()\n",
    "                output_dict[\"Output\"] = avg_vector\n",
    "            elif hook_layer == 'fc':\n",
    "                output_dict[\"Output\"] = resnet_output.cpu().numpy().squeeze()\n",
    "            \n",
    "            # Append the dictionary to the list\n",
    "            outputs_with_ids.append(output_dict)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing ImageID {i}: {e}\")\n",
    "            skipped_images.append(i)\n",
    "    # Remove the hook to free resources\n",
    "    hook.remove()\n",
    "    \n",
    "    return outputs_with_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8677/8677 [05:39<00:00, 25.57it/s]\n"
     ]
    }
   ],
   "source": [
    "output_avgpool_with_ids = resnet_computations('avgpool',dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8677\n"
     ]
    }
   ],
   "source": [
    "print(len(output_avgpool_with_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8677/8677 [05:38<00:00, 25.64it/s]\n"
     ]
    }
   ],
   "source": [
    "output_layers3_with_ids = resnet_computations('layer3',dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8677\n"
     ]
    }
   ],
   "source": [
    "print(len(output_layers3_with_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8677/8677 [05:40<00:00, 25.47it/s]\n"
     ]
    }
   ],
   "source": [
    "output_fc_with_ids = resnet_computations('fc',dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8677\n"
     ]
    }
   ],
   "source": [
    "print(len(output_fc_with_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD, NMF\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd \n",
    "# df_color_moments = pd.read_csv(\"color_moments_with_imageID.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(df_color_moments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_columns = df_color_moments.columns[1:-1]\n",
    "# data_color_moments = df_color_moments[feature_columns].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data_color_moments[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 5 \n",
    "# svd = TruncatedSVD(n_components = k)\n",
    "# latent_semantics = svd.fit_transform(data_color_moments)\n",
    "# print(\"Reduced Data:(Latent Semantics)\")\n",
    "# print(latent_semantics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVD_calc(feature_matrix,k):\n",
    "    #print(\"Enter SVD calc\")\n",
    "    svd = TruncatedSVD(n_components=k)\n",
    "    latent_semantics = svd.fit_transform(feature_matrix)\n",
    "    #print(\"The latent semantics are:\")\n",
    "    print(latent_semantics)\n",
    "    return latent_semantics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NMF_calculator(feature_matrix,k):\n",
    "    nmf = NMF(n_components=k)\n",
    "    W = nmf.fit_transform(feature_matrix)\n",
    "    H = nmf.components_\n",
    "    return H "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LDA_calculator(feature_matrix,k):\n",
    "    lda = LinearDiscriminantAnalysis(n_components = k)\n",
    "    lda_result = lda.fit_transform(feature_matrix,labels_caltech_101)\n",
    "    top_k_latent = lda.scalings_[:, :k]\n",
    "    return top_k_latent\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means_calculator(feature_matrix,k):\n",
    "    kmeans = KMeans(n_clusters = k,random_state=0)\n",
    "    kmeans.fit(feature_matrix)\n",
    "    top_k_latent_semantics = kmeans.cluster_centers_\n",
    "    return top_k_latent_semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below Is the Code Of Block of converting the Phase 1 results into feature matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd \n",
    "# df_color_moments = pd.read_csv(\"color_moments_with_imageID.csv\")\n",
    "# feature_columns = df_color_moments.columns[1:-1]\n",
    "# data_color_moments = df_color_moments[feature_columns].values\n",
    "# print(feature_columns)\n",
    "# print(data_color_moments[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the Below code block use data_color_moments as the feature matrix for SVD \n",
    "#### Use the X_color_moments for NMF and X_standardized_color_moments for remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd \n",
    "color_moments_df = pd.DataFrame(color_moments_list)\n",
    "color_moments_df.fillna(color_moments_df.mean(), inplace=True)\n",
    "data_color_moments= color_moments_df.drop(columns=\"ImageID\").to_numpy()\n",
    "n_grids_per_image = 10 * 10  # 10x10 grid for each image\n",
    "n_features_per_grid = data_color_moments.shape[1]\n",
    "X_color_moments = data_color_moments.reshape(len(dataset), n_grids_per_image * n_features_per_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_2 = StandardScaler()\n",
    "X_standardized_color_moments = scaler_2.fit_transform(X_color_moments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X_standardized_color_moments[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8677\n"
     ]
    }
   ],
   "source": [
    "print(len(X_color_moments))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use X_hog for nmf and X_hog_standardized for remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_hog = np.array(list(hog_features_dict.values()))\n",
    "scaler = StandardScaler()\n",
    "X_hog_standardized = scaler.fit_transform(X_hog)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X_hog_standardized[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Matrix Conversion of Resnet computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_resnet_avgpool = np.array([entry[\"Output\"] for entry in output_avgpool_with_ids])\n",
    "X_resnet_layers3 = np.array([entry[\"Output\"] for entry in output_layers3_with_ids])\n",
    "X_resnet_fc = np.array([entry[\"Output\"] for entry in output_fc_with_ids])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8677\n"
     ]
    }
   ],
   "source": [
    "print(len(X_resnet_layers3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use X_resnet_{layer_value} for NMF and X_standardized_resnet_{layer_value} for remaining reduction technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardizing the data \n",
    "scaler_1 = StandardScaler()\n",
    "X_standardized_resnet_avgpool = scaler_1.fit_transform(X_resnet_avgpool)\n",
    "X_standardized_resnet_layers3 = scaler_1.fit_transform(X_resnet_layers3)\n",
    "X_standardized_resnet_fc = scaler_1.fit_transform(X_resnet_fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8677\n"
     ]
    }
   ],
   "source": [
    "print(len(X_standardized_resnet_fc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X_standardized_resnet_fc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Create the UI here and call appropriate functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_model_data(feature_model):\n",
    "    if feature_model == \"Color Moments\":\n",
    "        X_data = X_color_moments\n",
    "        X_standardized = X_standardized_color_moments\n",
    "    elif feature_model == \"HOG Descriptor\":\n",
    "        X_data = X_hog\n",
    "        X_standardized = X_hog_standardized\n",
    "    elif  feature_model == \"Resnet FC\":\n",
    "        X_data = X_resnet_fc\n",
    "        X_standardized = X_standardized_resnet_fc\n",
    "    elif feature_model == \"Resnet Avgpool\":\n",
    "        X_data = X_resnet_avgpool\n",
    "        X_standardized= X_standardized_resnet_avgpool \n",
    "    elif feature_model == \"Resnet Layer 3\":\n",
    "        X_data = X_resnet_layers3\n",
    "        X_standardized= X_standardized_resnet_layers3\n",
    "    else:\n",
    "        return -1\n",
    "    return X_data,X_standardized\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dimensionality_reduction(X_data,X_standardized,technique,k):\n",
    "    if technique == 'SVD':\n",
    "        #print(\"Entered SVD\")\n",
    "        result = SVD_calc(X_standardized,k)\n",
    "    elif technique == 'NNMF':\n",
    "        result = NMF_calculator(X_data,k)\n",
    "    elif technique =='LDA':\n",
    "        result = LDA_calculator(X_standardized,k)\n",
    "    elif technique == 'k-means':\n",
    "        result = k_means_calculator(X_standardized,k)\n",
    "    else :\n",
    "        return -1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    #Select a Feature Model\n",
    "    # Prompt user for feature model\n",
    "    print(\"Choose a feature model:\")\n",
    "    print(\"1. Color Moments\")\n",
    "    print(\"2. HOG Descriptor\")\n",
    "    print(\"3. Resnet FC\")\n",
    "    print(\"4. Resnet Avgpool\")\n",
    "    print(\"5. Resnet Layer 3\")\n",
    "    feature_model_choice = input(\"Enter your choice(number): \")\n",
    "    # Convert choice to string name\n",
    "    feature_model = [\"Color Moments\", \"HOG Descriptor\", \"Resnet FC\", \"Resnet Avgpool\", \"Resnet Layer 3\"][int(feature_model_choice) - 1]\n",
    "    #print(feature_model)\n",
    "    X_data, X_standardized_data = get_feature_model_data(feature_model)\n",
    "#Select the Dimensionality Reduction Technique \n",
    "    k = int(input('Please Enter the value of k'))\n",
    "    # Prompt user for dimensionality reduction technique\n",
    "    print(\"\\nChoose a dimensionality reduction technique:\")\n",
    "    print(\"1. SVD\")\n",
    "    print(\"2. NNMF\")\n",
    "    print(\"3. LDA\")\n",
    "    print(\"4. k-means\")\n",
    "    technique_choice = input(\"Enter your choice: \")\n",
    "    #Convert choice to specified string \n",
    "    technique = [\"SVD\", \"NNMF\", \"LDA\", \"k-means\"][int(technique_choice) - 1]\n",
    "    result = dimensionality_reduction(X_data,X_standardized_data,technique,k,)\n",
    "    print(f\"top {k} latent-semantics of {feature_model} using {technique}\")\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose a feature model:\n",
      "1. Color Moments\n",
      "2. HOG Descriptor\n",
      "3. Resnet FC\n",
      "4. Resnet Avgpool\n",
      "5. Resnet Layer 3\n",
      "\n",
      "Choose a dimensionality reduction technique:\n",
      "1. SVD\n",
      "2. NNMF\n",
      "3. LDA\n",
      "4. k-means\n",
      "[[  0.058179     0.03328266  -4.63146497 ...  -5.92938044  -2.96327335\n",
      "   -1.64448583]\n",
      " [  7.88788642  -0.10018586   4.0832809  ...   1.34306705  -1.67159542\n",
      "    0.08110227]\n",
      " [ -6.19782064  -0.03761281  -1.14583625 ...  -5.4032408   -0.82234513\n",
      "   -1.75852333]\n",
      " ...\n",
      " [-10.06320898  -0.18002127   2.10457802 ...  -4.5335982    3.62428075\n",
      "   -0.37549492]\n",
      " [  1.26859816  13.25711803  -4.13699945 ...  22.73296103  -7.63288573\n",
      "  -31.15577838]\n",
      " [-13.59723828  -0.19151409   3.80929369 ... -12.86814475  -5.69114587\n",
      "   -0.81008176]]\n",
      "top 10 latent-semantics of Color Moments using SVD\n",
      "[[  0.058179     0.03328266  -4.63146497 ...  -5.92938044  -2.96327335\n",
      "   -1.64448583]\n",
      " [  7.88788642  -0.10018586   4.0832809  ...   1.34306705  -1.67159542\n",
      "    0.08110227]\n",
      " [ -6.19782064  -0.03761281  -1.14583625 ...  -5.4032408   -0.82234513\n",
      "   -1.75852333]\n",
      " ...\n",
      " [-10.06320898  -0.18002127   2.10457802 ...  -4.5335982    3.62428075\n",
      "   -0.37549492]\n",
      " [  1.26859816  13.25711803  -4.13699945 ...  22.73296103  -7.63288573\n",
      "  -31.15577838]\n",
      " [-13.59723828  -0.19151409   3.80929369 ... -12.86814475  -5.69114587\n",
      "   -0.81008176]]\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8677"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels_caltech_101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8677\n"
     ]
    }
   ],
   "source": [
    "print(len(hog_features_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hog_features_dict[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "867700\n"
     ]
    }
   ],
   "source": [
    "print(len(color_moments_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Red_Mean': 100.75,\n",
       " 'Red_Std': 41.33296706826969,\n",
       " 'Red_Skewness': 0.15939082768196947,\n",
       " 'Green_Mean': 99.26666666666667,\n",
       " 'Green_Std': 45.024166350478446,\n",
       " 'Green_Skewness': 0.17302136897815748,\n",
       " 'Blue_Mean': 93.96333333333334,\n",
       " 'Blue_Std': 46.95034954313144,\n",
       " 'Blue_Skewness': 0.11821055484499522,\n",
       " 'ImageID': 0}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "color_moments_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8677\n"
     ]
    }
   ],
   "source": [
    "print(len(output_avgpool_with_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output_avgpool_with_ids[435]['Output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8677\n"
     ]
    }
   ],
   "source": [
    "print(len(output_layers3_with_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8677\n"
     ]
    }
   ],
   "source": [
    "print(len(output_fc_with_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HOG TENSOR CREATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_empty_tensor(num_images, feature_length, num_labels):\n",
    "    \"\"\"\n",
    "    Creates an empty tensor filled with zeros.\n",
    "    \"\"\"\n",
    "    return np.zeros((num_images, feature_length, num_labels))\n",
    "\n",
    "def get_label_index(label, all_labels):\n",
    "    \"\"\"\n",
    "    Returns the index of the given label in the all_labels list.\n",
    "    \"\"\"\n",
    "    return all_labels.index(label)\n",
    "\n",
    "def construct_tensor(feature_vectors, labels_list, all_labels):\n",
    "    num_images = len(feature_vectors)\n",
    "    feature_length = len(next(iter(feature_vectors.values())))  # Length of a feature vector\n",
    "    num_labels = len(all_labels)\n",
    "\n",
    "    tensor = create_empty_tensor(num_images, feature_length, num_labels)\n",
    "\n",
    "    for idx, feature in enumerate(feature_vectors.values()):\n",
    "        # Set the feature vector in the tensor\n",
    "        tensor[idx, :, :] = np.array(feature)[:, np.newaxis]\n",
    "\n",
    "        # Set the label in the tensor\n",
    "        label = labels_list[idx]\n",
    "        label_index = get_label_index(label, all_labels)\n",
    "        tensor[idx, :, label_index] = 1\n",
    "\n",
    "    return tensor\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "hog_tensor = construct_tensor(hog_features_dict, labels_caltech_101, list(set(labels_caltech_101)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(hog_tensor, 'hog_tensor_file.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8677, 900, 101)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hog_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COLOR MOMENTS TENSOR CREATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_color_moments_tensor(data_list):\n",
    "    num_images = len(data_list) // 100\n",
    "    features_per_element = 9 # Max 9 for RGB\n",
    "    elements_per_image = 100\n",
    "\n",
    "    tensor = np.zeros((num_images, features_per_element * elements_per_image, len(set(labels_caltech_101))))\n",
    "\n",
    "    for i in range(num_images):\n",
    "        for j in range(elements_per_image):\n",
    "            element = data_list[i*100 + j]\n",
    "            if 'Red_Mean' in element:  # RGB features\n",
    "                features = [element[key] for key in ['Red_Mean', 'Red_Std', 'Red_Skewness',\n",
    "                                                     'Green_Mean', 'Green_Std', 'Green_Skewness',\n",
    "                                                     'Blue_Mean', 'Blue_Std', 'Blue_Skewness']]\n",
    "            else:  # Grayscale features\n",
    "                features = [element[key] for key in ['Gray_Mean', 'Gray_Std', 'Gray_Skewness']]\n",
    "                # Padding to match the RGB feature length\n",
    "                features.extend([0] * (9 - len(features)))\n",
    "            \n",
    "            start_idx = j * features_per_element\n",
    "            end_idx = start_idx + features_per_element\n",
    "            tensor[i, start_idx:end_idx, j] = features\n",
    "\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8677, 900, 101)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "color_moments_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_moments_tensor = construct_color_moments_tensor(color_moments_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a tensor\n",
    "\n",
    "index_to_extract = 0\n",
    "extracted_tensor = color_moments_tensor[index_to_extract, :, :]\n",
    "\n",
    "# Printing the entire 2D tensor without truncation by iterating through its rows and columns\n",
    "output_data = []\n",
    "for row in extracted_tensor:\n",
    "    output_data.append(list(row))\n",
    "\n",
    "# output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RESNET TENSOR CREATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8677"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels_caltech_101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_resnet_tensor(data_list):\n",
    "    num_images = len(data_list)\n",
    "    feature_length = len(data_list[0]['Output'])\n",
    "    \n",
    "    tensor = np.zeros((num_images, feature_length, len(set(labels_caltech_101))))\n",
    "\n",
    "    for i, element in enumerate(data_list):\n",
    "        features = element['Output']\n",
    "        label_index = labels_caltech_101[i]\n",
    "        tensor[i, :, label_index] = features\n",
    "\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_avgpool_tensor = construct_resnet_tensor(output_avgpool_with_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8677, 1024, 101)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_avgpool_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_layer3_tensor = construct_resnet_tensor(output_layers3_with_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8677, 1024, 101)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_layer3_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_fc_tensor = construct_resnet_tensor(output_fc_with_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8677, 1000, 101)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_fc_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main function for CP decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorly as tl\n",
    "from tensorly.decomposition import parafac\n",
    "\n",
    "feature_tensor = {\n",
    "    1: hog_tensor,\n",
    "    2: color_moments_tensor,\n",
    "    3: resnet_avgpool_tensor,\n",
    "    4: resnet_layer3_tensor,\n",
    "    5: resnet_fc_tensor\n",
    "}\n",
    "\n",
    "# 1: HOG\n",
    "# 2: Color Moments\n",
    "# 3: Resnet Avgpool\n",
    "# 4: Resnet Layer3\n",
    "# 5: Resnet FC\n",
    "\n",
    "def extract_latent_semantics(tensor, k, labels):\n",
    "    # Perform CP-decomposition\n",
    "    weights, factors = parafac(tensor, rank=k)\n",
    "    \n",
    "    # Extract the weights for the label mode (assuming it's the last mode)\n",
    "    label_weights = factors[2]\n",
    "    \n",
    "    # Sort the weights and associate with labels\n",
    "    sorted_indices = label_weights.argsort(axis=0)[::-1].flatten()\n",
    "    sorted_labels = [labels[i] for i in sorted_indices]\n",
    "    sorted_weights = label_weights[sorted_indices].flatten()\n",
    "    \n",
    "    # Prepare the label-weight pairs\n",
    "    label_weight_pairs = list(zip(sorted_labels, sorted_weights))\n",
    "    \n",
    "    return label_weight_pairs\n",
    "\n",
    "def save_to_file(label_weight_pairs, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        for label, weight in label_weight_pairs:\n",
    "            f.write(f\"{label}: {weight}\\n\")\n",
    "\n",
    "def main_task_4(feature_model_id, k):\n",
    "    tensor = feature_tensor[feature_model_id]\n",
    "    labels = set(labels_caltech_101)\n",
    "    \n",
    "    latent_semantics = extract_latent_semantics(tensor, k, labels)\n",
    "    save_to_file(latent_semantics, \"latent_semantics_output.txt\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import TruncatedSVD, NMF, LatentDirichletAllocation\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "def create_label_similarity_matrix(tensor):\n",
    "    # Summing across the image and feature dimensions to get label representations\n",
    "    label_representations = np.sum(tensor, axis=(0, 1))\n",
    "    \n",
    "    # Compute pairwise Euclidean distances\n",
    "    distances = pdist(label_representations.reshape(-1, 1), metric='euclidean')\n",
    "    square_distances = squareform(distances)\n",
    "    \n",
    "    # Convert distances to similarities\n",
    "    similarities = 1 / (1 + square_distances)\n",
    "    \n",
    "    return similarities\n",
    "\n",
    "\n",
    "def dimensionality_reduction(matrix, method, k):\n",
    "    if method == \"SVD\":\n",
    "        svd = TruncatedSVD(n_components=k)\n",
    "        reduced_matrix = svd.fit_transform(matrix)\n",
    "    elif method == \"NNMF\":\n",
    "        nmf = NMF(n_components=k)\n",
    "        reduced_matrix = nmf.fit_transform(matrix)\n",
    "    elif method == \"LDA\":\n",
    "        lda = LatentDirichletAllocation(n_components=k)\n",
    "        reduced_matrix = lda.fit_transform(matrix)\n",
    "    elif method == \"k-means\":\n",
    "        kmeans = KMeans(n_clusters=k)\n",
    "        reduced_matrix = kmeans.fit_transform(matrix)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "    return reduced_matrix\n",
    "\n",
    "def save_to_file(matrix, filename):\n",
    "    np.savetxt(filename, matrix)\n",
    "\n",
    "def main_task_5(feature_model_id, k, reduction_method):\n",
    "    tensor = feature_tensor[feature_model_id]\n",
    "    similarity_matrix = create_label_similarity_matrix(tensor)\n",
    "    reduced_matrix = dimensionality_reduction(similarity_matrix, reduction_method, k)\n",
    "    \n",
    "    # Save latent semantics\n",
    "    # filename = f\"latent_semantics_{reduction_method}.txt\"\n",
    "    # save_to_file(reduced_matrix, filename)\n",
    "    \n",
    "    # List label-weight pairs\n",
    "    label_weights = np.sum(reduced_matrix, axis=1)\n",
    "    sorted_indices = label_weights.argsort()[::-1]\n",
    "    sorted_labels = [labels_caltech_101[i] for i in sorted_indices]\n",
    "    sorted_weights = label_weights[sorted_indices]\n",
    "    label_weight_pairs = list(zip(sorted_labels, sorted_weights))\n",
    "    return label_weight_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 5.966975521739771),\n",
       " (0, 5.9668464785790585),\n",
       " (0, 5.449396927653185),\n",
       " (0, 5.379511073161442),\n",
       " (0, 5.314395832865798),\n",
       " (0, 5.314180414053493),\n",
       " (0, 5.283544098868186),\n",
       " (0, 5.1676549259559685),\n",
       " (0, 5.167262695217472),\n",
       " (0, 5.154901449934496),\n",
       " (0, 5.142179600113071),\n",
       " (0, 5.141815097706982),\n",
       " (0, 5.133609249237595),\n",
       " (0, 5.13118401831645),\n",
       " (0, 5.127953904826918),\n",
       " (0, 5.121690857935883),\n",
       " (0, 5.117607868995081),\n",
       " (0, 5.1138607850439115),\n",
       " (0, 5.111837764249434),\n",
       " (0, 5.111575192674),\n",
       " (0, 5.111322033266526),\n",
       " (0, 5.110638923147751),\n",
       " (0, 5.108204299068233),\n",
       " (0, 5.108160202227085),\n",
       " (0, 5.104044128507998),\n",
       " (0, 5.102041187321486),\n",
       " (0, 5.101201397588099),\n",
       " (0, 5.101131116028064),\n",
       " (0, 5.1009156189672495),\n",
       " (0, 5.099940027769826),\n",
       " (0, 5.0999302747466215),\n",
       " (0, 5.099703244923324),\n",
       " (0, 5.099683865370208),\n",
       " (0, 5.099118392125359),\n",
       " (0, 5.098757118344783),\n",
       " (0, 5.097307610699725),\n",
       " (0, 5.097065903979004),\n",
       " (0, 5.09680978550564),\n",
       " (0, 5.096242157670581),\n",
       " (0, 5.0960438934395835),\n",
       " (0, 5.095432838575937),\n",
       " (0, 5.0951224962501644),\n",
       " (0, 5.094804790874101),\n",
       " (0, 5.09425402767548),\n",
       " (0, 5.093927933282564),\n",
       " (0, 5.093871316535132),\n",
       " (0, 5.093766952679104),\n",
       " (0, 5.093649095049692),\n",
       " (0, 5.093499058743921),\n",
       " (0, 5.093344298698911),\n",
       " (0, 5.0933042280612755),\n",
       " (0, 5.093268415699034),\n",
       " (0, 5.093228210328482),\n",
       " (0, 5.092903678054455),\n",
       " (0, 5.092064145084353),\n",
       " (0, 5.091911447326296),\n",
       " (0, 5.091861761214561),\n",
       " (0, 5.091777844094109),\n",
       " (0, 5.091128352810554),\n",
       " (0, 5.090340218078635),\n",
       " (0, 5.089878275218349),\n",
       " (0, 5.088645211778579),\n",
       " (0, 5.087965978872115),\n",
       " (0, 5.087745185983158),\n",
       " (0, 5.087653087104681),\n",
       " (0, 5.087526187063475),\n",
       " (0, 5.087364837600732),\n",
       " (0, 5.0869676702907185),\n",
       " (0, 5.083931244700206),\n",
       " (0, 5.081584558681281),\n",
       " (0, 5.080071939680287),\n",
       " (0, 5.078878586250938),\n",
       " (0, 5.078217019546236),\n",
       " (0, 5.07818256485743),\n",
       " (0, 5.077416427497975),\n",
       " (0, 5.074133143049454),\n",
       " (0, 5.074001208589803),\n",
       " (0, 5.073595684641795),\n",
       " (0, 5.0722518865483295),\n",
       " (0, 5.070857127346658),\n",
       " (0, 5.0701860071087586),\n",
       " (0, 5.067302229272702),\n",
       " (0, 5.0665627500048185),\n",
       " (0, 5.0650044479800265),\n",
       " (0, 5.055157541195683),\n",
       " (0, 5.038655272303398),\n",
       " (0, 5.019931257691202),\n",
       " (0, 5.0191686258344825),\n",
       " (0, 5.000381895672067),\n",
       " (0, 4.994746959171669),\n",
       " (0, 4.985759011457629),\n",
       " (0, 4.984394657058219),\n",
       " (0, 4.969464029939788),\n",
       " (0, 4.956597276647013),\n",
       " (0, 4.949678288254841),\n",
       " (0, 4.948633856360879),\n",
       " (0, 4.93294058087491),\n",
       " (0, 4.314783782387469),\n",
       " (0, 4.314743501492719),\n",
       " (0, 4.265777664402407),\n",
       " (0, 4.265497832499047)]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_task_5(4, 4, \"k-means\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
